# CARLA Autopilot Multimodal LLM/Vision Transformer Model

A sophisticated autonomous driving system that integrates Vision Transformer models, speech recognition, and Large Language Models (LLMs) to enable intelligent vehicle control in the CARLA simulator. The system processes multimodal inputs (camera images and audio commands) to generate real-time driving instructions.

---

## üéØ Project Overview

This project demonstrates a **multimodal AI approach** to autonomous driving by combining:

- **Vision Transformer (ViT)** for scene understanding from camera images
- **Speech Recognition** for processing voice commands
- **Gemini LLM** for intelligent decision-making based on multimodal inputs
- **CARLA Simulator** for realistic autonomous driving simulation

The system generates driving instructions (speed and steering) by analyzing both visual scene information and audio commands, enabling more intuitive and context-aware autonomous driving.

---

## üèóÔ∏è Architecture

### High-Level System Architecture

```
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ    CARLA Simulator          ‚îÇ
                    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
                    ‚îÇ  ‚îÇ Vehicle ‚îÇ ‚îÇ Camera   ‚îÇ  ‚îÇ
                    ‚îÇ  ‚îÇ Control ‚îÇ ‚îÇ Sensor   ‚îÇ  ‚îÇ
                    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ           ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
                    ‚îÇ  RGB Image    ‚îÇ   ‚îÇ
                    ‚îÇ  (224x224)    ‚îÇ   ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
                            ‚îÇ           ‚îÇ
                            ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                            ‚îÇ   ‚îÇ  Audio File   ‚îÇ
                            ‚îÇ   ‚îÇ  (audio.mp3)  ‚îÇ
                            ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ           ‚îÇ
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ   Multimodal Processing Pipeline          ‚îÇ
            ‚îÇ                                           ‚îÇ
            ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
            ‚îÇ  ‚îÇ Vision       ‚îÇ    ‚îÇ Audio        ‚îÇ    ‚îÇ
            ‚îÇ  ‚îÇ Transformer  ‚îÇ    ‚îÇ Processing   ‚îÇ    ‚îÇ
            ‚îÇ  ‚îÇ              ‚îÇ    ‚îÇ              ‚îÇ    ‚îÇ
            ‚îÇ  ‚îÇ Image‚ÜíText   ‚îÇ    ‚îÇ Audio‚ÜíText   ‚îÇ    ‚îÇ
            ‚îÇ  ‚îÇ Scene Desc.  ‚îÇ    ‚îÇ Speech Rec.  ‚îÇ    ‚îÇ
            ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
            ‚îÇ         ‚îÇ                   ‚îÇ             ‚îÇ
            ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ
            ‚îÇ                   ‚îÇ                       ‚îÇ
            ‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ
            ‚îÇ         ‚îÇ  Gemini LLM       ‚îÇ             ‚îÇ
            ‚îÇ         ‚îÇ  Decision Engine  ‚îÇ             ‚îÇ
            ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ
            ‚îÇ                   ‚îÇ                       ‚îÇ
            ‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ
            ‚îÇ         ‚îÇ  JSON Output      ‚îÇ             ‚îÇ
            ‚îÇ         ‚îÇ  {speed, steer}   ‚îÇ             ‚îÇ
            ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                      ‚îÇ  Vehicle Control  ‚îÇ
                      ‚îÇ  Speed/Steering   ‚îÇ
                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Component Details

#### 1. Vision Transformer Model

**Files**: `camera_text_processing.py`, `custom_layers.py`

**Purpose**: Converts camera images to textual scene descriptions

**Architecture**:

- **Input**: RGB images (224x224x3)
- **Patch Size**: 16x16 pixels
- **Number of Patches**: 144 (14x14 patches from 224x224 image)
- **Projection Dimension**: 64
- **Custom Layers**:
  - `Patches`: Extracts image patches
  - `PatchEncoder`: Encodes patches with positional embeddings

**Output Format**:

- Textual scene description with confidence scores
- Example: "Scene contains: road ahead clear (confidence: 0.85), vehicles nearby (confidence: 0.72)"

**Model Location**: `vision_transformer_model/model/my_model.keras`

**Training Data**: CARLA-specific dataset from HuggingFace

#### 2. Audio Processing Module

**File**: `audio_conversion.py`

**Purpose**: Converts audio files to text using speech recognition

**Pipeline**:

1. **Audio Input**: MP3 file (or other formats via pydub)
2. **Format Conversion**: Convert to WAV using FFmpeg
3. **Speech Recognition**: Google Speech Recognition API
4. **Text Output**: Transcribed command text

**Dependencies**:

- `pydub`: Audio format conversion
- `speech_recognition`: Speech-to-text conversion
- `ffmpeg`: Audio processing (external dependency)

**Error Handling**:

- Returns mock text if audio file not found
- Handles API errors gracefully

#### 3. LLM Decision Engine

**File**: `text_to_instructions_converter.py`

**Purpose**: Synthesizes multimodal inputs to generate driving commands

**Model**: Google Gemini 2.5 Flash

**Input**:

- Scene description text (from Vision Transformer)
- Audio command text (from speech recognition)
- Current vehicle state:
  - Current speed (m/s)
  - Current steering angle (-1.0 to 1.0)

**Prompt Structure**:

```
Analyze the following driving scenario and provide the required speed and steer values.
You are an autonomous vehicle control system. Your objective is to follow the road while maintaining the target speed.

[Audio text]
[Scene description]

Current vehicle state:
Speed: X m/s
Steer: Y

Target state:
Target Speed: X m/s

Instruction:
Generate a JSON object with two keys: "speed" (float in m/s, typically 3-15 m/s) and "steer" (float between -1.0 and 1.0).
```

**Output Format**: JSON

```json
{
  "speed": 5.0,
  "steer": 0.1
}
```

**Validation**:

- Speed clamped to 3-20 m/s
- Steering clamped to -1.0 to 1.0
- Fallback to current values on error

#### 4. Vehicle Controller

**Files**: `player.py`, `controller.py`

**Purpose**: Implements low-level vehicle control using PID controllers

**Architecture**:

- **Lateral Control**: PID controller for steering
  - K_P: 1.95
  - K_I: 0.05
  - K_D: 0.2
- **Longitudinal Control**: PID controller for speed
  - K_P: 1.0
  - K_I: 0.05
  - K_D: 0

**Control Modes**:

- Waypoint following
- Lane changing (left/right)
- Speed control

**Safety Features**:

- Maximum throttle: 0.75
- Maximum brake: 0.3
- Maximum steering: 0.8
- Steering rate limiting (prevents abrupt changes)

#### 5. Simulation Loop

**File**: `simulator.py`

**Purpose**: Main orchestration of the autonomous driving system

**Flow**:

1. **Initialization**:

   - Connect to CARLA server
   - Spawn vehicle
   - Setup camera sensor
   - Initialize processing modules

2. **Main Loop** (per frame):

   - Tick simulation world
   - Capture camera image
   - Process audio (if available)
   - Get scene description from Vision Transformer
   - Generate control commands from LLM
   - Apply vehicle control
   - Update spectator camera
   - Send metrics to dashboard

3. **Collision Detection**:

   - Monitor angular velocity
   - Detect sudden stops
   - Track collision count

4. **Cleanup**:
   - Destroy sensors
   - Destroy actors
   - Close connections

**Configuration**:

- Configurable via environment variables
- Frame limit for testing
- Adjustable timeouts

#### 6. Dashboard

**Location**: `vision-transformer-dashboard/`

**Components**:

- **Frontend** (Angular):
  - Simulation metrics visualization
  - Bar charts, pie charts, gauge charts
  - Real-time data updates (refreshes every 2 seconds)
- **Backend** (Express.js):
  - REST API for simulation data
  - Endpoint: `/api/simulations`
  - Stores collision and safety metrics

**Data Flow**:

```
Simulator ‚Üí HTTP POST ‚Üí Dashboard Backend ‚Üí Frontend Display
```

### Data Flow

#### Complete Processing Pipeline

```
1. CARLA Simulation
   ‚îî‚îÄ> Camera captures RGB image (224x224x3)
   ‚îî‚îÄ> Audio file provided (audio.mp3)

2. Vision Processing
   ‚îî‚îÄ> Image normalized (0-1 range)
   ‚îî‚îÄ> Vision Transformer inference
   ‚îî‚îÄ> Scene description text generated

3. Audio Processing
   ‚îî‚îÄ> Audio converted to WAV
   ‚îî‚îÄ> Speech recognition applied
   ‚îî‚îÄ> Command text extracted

4. LLM Decision
   ‚îî‚îÄ> Prompt constructed with:
       - Scene description
       - Audio command
       - Current vehicle state
   ‚îî‚îÄ> Gemini generates JSON response
   ‚îî‚îÄ> Speed and steering values extracted

5. Control Application
   ‚îî‚îÄ> Values validated and clamped
   ‚îî‚îÄ> Rate limiting applied
   ‚îî‚îÄ> Vehicle control executed
   ‚îî‚îÄ> Metrics sent to dashboard

6. Loop continues...
```

### Design Decisions

#### Why Vision Transformer?

- **Attention Mechanism**: Effective for scene understanding
- **Custom Training**: Trained on CARLA-specific data for better performance
- **Text Output**: Generates interpretable scene descriptions

#### Why Gemini LLM?

- **Multimodal Capabilities**: Can process text from multiple sources
- **JSON Output**: Structured output for reliable parsing
- **Fast Inference**: Gemini 2.5 Flash optimized for speed

#### Why Multimodal Approach?

- **Robustness**: Multiple information sources reduce errors
- **Flexibility**: Can handle voice commands and visual analysis
- **Natural Interaction**: Voice commands are more intuitive

#### Safety Constraints

- **Speed Limits**: 3-15 m/s for urban driving
- **Steering Limits**: -1.0 to 1.0 (full range)
- **Rate Limiting**: Prevents abrupt steering changes
- **Validation**: All LLM outputs validated before application

### Performance Characteristics

#### Inference Times (Estimated)

- Vision Transformer: ~50-100ms per frame
- Audio Processing: ~200-500ms (network dependent)
- LLM Inference: ~200-500ms (network dependent)
- **Total Pipeline**: ~500-1000ms per frame

#### Resource Requirements

- **GPU**: Optional (TensorFlow can use CPU)
- **RAM**: ~2-4 GB
- **Network**: Required for Google APIs (Speech Recognition, Gemini)
- **Disk**: ~500 MB (model + dependencies)

---

## üìä Methodology

### Approach

Our system uses a **multimodal fusion strategy** where:

1. **Visual Analysis**: The Vision Transformer analyzes camera frames and generates scene descriptions (e.g., "road ahead clear", "vehicles nearby", "obstacles detected")

2. **Audio Processing**: Voice commands are converted to text using Google Speech Recognition API

3. **Contextual Decision Making**: The Gemini LLM receives both:

   - Scene description from the Vision Transformer
   - Audio command text
   - Current vehicle state (speed, steering angle)

   And generates appropriate speed and steering values as JSON output.

4. **Safety Constraints**: Generated commands are validated and clamped to safe ranges:
   - Speed: 3-15 m/s for urban driving
   - Steering: -1.0 to 1.0 (full left to full right)
   - Rate limiting to prevent abrupt changes

### Vision Transformer Architecture

- **Patch Size**: 16x16 pixels
- **Projection Dimension**: 64
- **Number of Patches**: 144 (for 224x224 input images)
- **Input Shape**: 224x224x3 RGB images
- **Output**: Scene description text with confidence scores

---

## üìà Results & Evaluation

### Performance Metrics

The system has been tested on CARLA's Town03 map with the following characteristics:

- **Simulation Environment**: CARLA Town03 (urban environment)
- **Weather Conditions**: Clear noon (optimized for visibility)
- **Vehicle**: Audi e-tron
- **Test Scenarios**: Urban driving with multiple lanes and traffic

### Key Capabilities

‚úÖ **Multimodal Input Processing**: Successfully processes both visual and audio inputs  
‚úÖ **Real-time Decision Making**: Generates control commands at ~20 FPS  
‚úÖ **Collision Detection**: Monitors and reports collisions during simulation  
‚úÖ **Dashboard Integration**: Real-time visualization of simulation metrics  
‚úÖ **Safety Constraints**: Enforces speed and steering limits for safe operation

### Limitations & Future Work

**Current Limitations:**

- Limited to single vehicle scenarios
- Requires CARLA server to be running
- Audio processing requires internet connection (Google Speech Recognition)
- Vision model trained on specific CARLA dataset (may not generalize)

**Future Improvements:**

- [ ] Multi-agent scenarios with traffic
- [ ] Offline speech recognition option
- [ ] Real-time model fine-tuning
- [ ] Extended testing across multiple CARLA maps
- [ ] Performance benchmarking vs. CARLA's built-in autopilot
- [ ] Ablation studies on component contributions
- [ ] Integration with additional sensors (LiDAR, radar)

### Extension Points

#### Adding New Sensors

1. Create sensor processing module (similar to `camera_text_processing.py`)
2. Add output to LLM prompt
3. Update main simulation loop

#### Adding New Models

1. Implement model loading in processing module
2. Add output format conversion
3. Integrate into decision pipeline

#### Custom Control Strategies

1. Modify `text_to_instructions_converter.py` prompt
2. Add custom validation logic
3. Implement new control modes in `player.py`

---

## üöÄ Setup Instructions

### Prerequisites

- Python 3.12+
- CARLA Simulator 0.9.16 (or compatible version)
- Node.js 18+ (for dashboard)
- FFmpeg (for audio processing)
- Google API key (for Gemini LLM)

### Step 1: Clone Repository

```bash
git clone https://github.com/SumukhP-dev/CARLA_Autopilot_LLM.git
cd CARLA_Autopilot_LLM
```

### Step 2: Install Python Dependencies

```bash
pip install -r requirements.txt
```

**Note**: If you encounter CARLA import issues, you may need to install CARLA Python API separately. See `setup_carla.py` for platform-specific setup.

### Step 3: Install FFmpeg

**Windows:**

- Download from [ffmpeg.org](https://ffmpeg.org/download.html)
- Extract to `C:\ffmpeg\` or update paths in `.env` file

**Linux/Mac:**

```bash
# Ubuntu/Debian
sudo apt-get install ffmpeg

# macOS
brew install ffmpeg
```

### Step 4: Configure Environment Variables

Copy the example environment file and fill in your values:

```bash
cp .env.example .env
```

Edit `.env` and set:

- `GOOGLE_API_KEY`: Your Google Gemini API key
- `CARLA_PATH`: Path to CARLA Python API (if not using default)
- `FFMPEG_PATH`: Path to FFmpeg executables (Windows only)
- `CARLA_HOST`: CARLA server host (default: localhost)
- `CARLA_PORT`: CARLA server port (default: 2000)

### Step 5: Start CARLA Simulator

1. Launch CARLA server:

   ```bash
   # Windows
   CarlaUE4.exe

   # Linux
   ./CarlaUE4.sh
   ```

2. Wait for the CARLA window to fully load

### Step 6: Run the Simulation

```bash
python simulator.py
```

### Step 7: (Optional) Start Dashboard

**Terminal 1 - Backend:**

```bash
cd vision-transformer-dashboard/collision-risk-backend
npm install
npm start
```

**Terminal 2 - Frontend:**

```bash
cd vision-transformer-dashboard
npm install
ng serve
```

Open `http://localhost:4200` to view the dashboard.

---

## üìÅ Project Structure

```
CARLA_Autopilot_LLM/
‚îú‚îÄ‚îÄ simulator.py                    # Main simulation loop
‚îú‚îÄ‚îÄ camera_text_processing.py       # Vision Transformer integration
‚îú‚îÄ‚îÄ audio_conversion.py             # Speech-to-text conversion
‚îú‚îÄ‚îÄ text_to_instructions_converter.py # LLM decision engine
‚îú‚îÄ‚îÄ player.py                       # Vehicle controller
‚îú‚îÄ‚îÄ controller.py                   # PID controller implementation
‚îú‚îÄ‚îÄ custom_layers.py                # Vision Transformer custom layers
‚îú‚îÄ‚îÄ requirements.txt                # Python dependencies
‚îú‚îÄ‚îÄ env.example                     # Environment variables template
‚îú‚îÄ‚îÄ vision_transformer_model/
‚îÇ   ‚îú‚îÄ‚îÄ model/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ my_model.keras          # Trained Vision Transformer model
‚îÇ   ‚îú‚îÄ‚îÄ datasets/                   # Training/test datasets
‚îÇ   ‚îî‚îÄ‚îÄ notebook/                   # Model training notebook
‚îî‚îÄ‚îÄ vision-transformer-dashboard/   # Angular dashboard
    ‚îú‚îÄ‚îÄ src/app/                    # Frontend components
    ‚îî‚îÄ‚îÄ collision-risk-backend/    # Express.js backend
```

---

## üé¨ Demo & Screenshots

### Screenshots

![Screenshot Placeholder](screenshots/0000.jpg)
![Screenshot Placeholder](screenshots/0434.jpg)
![Screenshot Placeholder](screenshots/dashboard-1.png)
![Screenshot Placeholder](screenshots/dashboard-2.png)

### Video Demo

_[Add link to demo video here when available]_

---

## üîó Important Links

- [Data Generation Repository](https://github.com/SumukhP-dev/Carla-Lane-Detection-Dataset-Generation)
- [Huggingface Image To Text Dataset](https://huggingface.co/datasets/Sumukhdev/carla_image_to_text_dataset)
- [Huggingface Image Captioning Model](https://huggingface.co/Sumukhdev/carla_image_captioning_model)
- [Research Paper](paper/paper.tex) - Technical report (LaTeX source)

## üìÑ Research Paper

A research paper template is available in `paper/paper.tex` formatted for academic submission. The paper describes the multimodal AI approach, methodology, and system architecture. See `RESEARCH_PAPER_GUIDE.md` for details on using the paper for grad school applications.

**Benefits:**

- Demonstrates academic writing skills
- Professional presentation of research contribution
- Can be included in grad school application portfolios
- Potential for workshop/conference submission

To compile the paper:

```bash
cd paper
pdflatex paper.tex
# (multiple passes needed for references)
```

---

## üõ†Ô∏è Tech Stack

- **Language**: Python 3.12+
- **Deep Learning**: TensorFlow/Keras, Vision Transformer
- **LLM**: Google Gemini 2.5 Flash
- **Simulation**: CARLA 0.9.16
- **Frontend**: Angular 20
- **Backend**: Express.js, Node.js
- **Testing**: Jasmine/Karma
- **Audio Processing**: Google Speech Recognition, pydub
- **Computer Vision**: OpenCV

---

## üìù Features

### Core Features

- ‚úÖ Multimodal AI pipeline (Vision + Audio + LLM)
- ‚úÖ Real-time autonomous driving simulation
- ‚úÖ Collision detection and safety monitoring
- ‚úÖ Interactive dashboard for visualization
- ‚úÖ Custom Vision Transformer model
- ‚úÖ Voice command processing
- ‚úÖ Intelligent decision-making with LLM

### What You'll See

- A main car navigating traffic without crashing
- Multiple lanes with lane-changing capabilities
- Real-time collision detection
- Dashboard showing simulation metrics
- AI-generated driving instructions based on scene analysis

---

## üß™ Testing

### Running Tests

**Frontend Tests:**

```bash
cd vision-transformer-dashboard
ng test
```

**Backend Tests:**

```bash
cd vision-transformer-dashboard/collision-risk-backend
npm test
```

### Manual Testing

1. Test CARLA connection:

   ```bash
   python test_carla_connection.py
   ```

2. Test model loading:
   ```bash
   python -c "from camera_text_processing import *; print('Model loaded successfully')"
   ```

---

## ü§ù Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## üìÑ License

This project is licensed under the MIT License - see the LICENSE file for details.

---

## üë§ Author

**Sumukh Paspuleti**

- [LinkedIn](https://www.linkedin.com/in/sumukh-paspuleti/)
- [Email](mailto:spaspuleti3@gatech.edu)
- [GitHub](https://github.com/SumukhP-dev)

---

## üôè Acknowledgments

- CARLA Simulator team for the excellent autonomous driving platform
- Google for Gemini API and Speech Recognition services
- HuggingFace for model hosting infrastructure
- The open-source community for various tools and libraries

---

## üìö References

- [CARLA Documentation](https://carla.readthedocs.io/)
- [Vision Transformer Paper](https://arxiv.org/abs/2010.11929)
- [Gemini API Documentation](https://ai.google.dev/docs)
- [Angular Documentation](https://angular.dev)

---

## ‚ö†Ô∏è Troubleshooting

### CARLA Import Errors

- Ensure CARLA is properly installed
- Check that `CARLA_PATH` in `.env` points to the correct location
- Run `python setup_carla.py` for platform-specific fixes

### Model Loading Errors

- Verify `vision_transformer_model/model/my_model.keras` exists
- Check that TensorFlow is properly installed
- Ensure custom layers are importable

### Audio Processing Errors

- Verify FFmpeg is installed and paths are correct
- Check internet connection for Google Speech Recognition
- Ensure audio file exists at specified path

### Dashboard Connection Issues

- Ensure backend is running on port 4000
- Check CORS settings if accessing from different origin
- Verify environment variables are set correctly

For more details, open an issue on GitHub.
